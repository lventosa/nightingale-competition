---
title: "Florence Nightingale Competition 2020 - RLadies Spain"
author: "Authors: Laura Ventosa and Esther Manzano"
date: "July 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---
```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
``` 

*****
# Introduction
*****

**Florence Nightingale** (1820-1910), known as *The Lady With the Lamp*, was a British nurse, social reformer and statistician best known as the founder of modern nursing. Her experiences as a nurse during the Crimean War were foundational in her views about sanitation. She established St. Thomas’ Hospital and the Nightingale Training School for Nurses in 1860. Her efforts to reform healthcare greatly influenced the quality of care in the 19 and 20 centuries.

In 1854, she and 38 other nurses trained by herself traveled to the Crimean peninsula to treat the wounded British soldiers in the Crimean War (October 1853 - March 1856). After their arrival, the nurses discovered that the treatments the injured soldiers were receiving were inaccurate and that the overall hygenic conditions and medical supplies were insufficient. Florence kept a record of the monthly deaths by injuries, zymotic diseases and other causes among the soldiers between April 1854 and March 1856. These data were going to be later used for her rose diagrams.

One of her most important discoveries was the fact that "deaths caused by diseases were more than seven times the number of deaths due to combat", because of unsanitary hospital conditions, as mentioned. However, knowing numbers alone have limited persuasive powers, Nightingale used her skills in statistical communication to convince the British parliament of the need to act. She avoided the dry tables used by most statisticians of the time, and instead devised a novel graph to illustrate the impact of hospital and nursing practice reform on army mortality rates.


*****
# Data Exploration
*****

We check the different types of data on our dataset. The aim is to understand the nature of our variables (i.e. the proportion of numerical and categorical variables) to be able to develope a better model afterwards. Checks on missing values and outliers will be performed in addition.

```{r}
rm(list=ls()) 

#Libraries used across the whole project:
library(readxl)
library(dplyr)
library(tidyverse)
library(gt)
library(dygraphs)
library(formattable)
library(xts) 
library(lmtest)
library(tseries)
library(sandwich)
library(forecast)
library(ResourceSelection)
library(pROC)
library(reshape2)
```
```{r}
#Importing dataset:
data <- read_excel("../nightingale-competition/datos_florence.xlsx", skip=1)
```

## Basic Information 

The following table shows the first rows of our dataset, including the names of the variables we have worked with.

```{r}
#Basic information:
head(data)
```
```{r}
#Dimensions of the dataset:
dim(data)
```

We can observe that the dimensions of our original dataset are 24x8. That is, 24 rows and 8 columns.

```{r}
#Variables:
sapply(data, class)
```
```{r}
summary(data)
```

Nightingale's original dataset consists of the following variables:

- Month (1): Character variable stating the month and year (period).

- Average size of army (2): Numeric variable that informs the reader about the average size of the army in such period of time.

- Zymotic diseases (3): Numeric variable which states the number of soldiers who died in such period of time for zymotic disease causes in absolute values.

- Wounds & injuries (4): Numeric variable which states the number of soldiers who died in such period of time for wounds and injuries causes in absolute values.

- All other causes (5): Numeric variable which states the number of soldiers who died in such period of time for any other causes in absolute values.

- Zymotic diseases (6): Numeric variable which states the number of soldiers who died in such period of time for zymotic disease causes in rate values.

- Wounds & injuries (7): Numeric variable which states the number of soldiers who died in such period of time for wounds and injuries causes in rate values.

- All other causes (8): Numeric variable which states the number of soldiers who died in such period of time for any other causes in rate values.

In order to make the data more readeable and understandable, we will make some changes in the variables as well as some variable additions.

```{r}
#Making everything more readable and column names more manageable:
colnames(data)[1] <- "month"
colnames(data)[2] <- "avg_size_army"
colnames(data)[3] <- "zymotic"
colnames(data)[4] <- "injuries"
colnames(data)[5] <- "other"
colnames(data)[6] <- "zymotic_rate"
colnames(data)[7] <- "injuries_rate"
colnames(data)[8] <- "other_rate"
```

## Missing Values and Outliers

We are interested in knowing whether we have missing observations before start treating our data.

```{r}
#Checking missing values:
colSums(is.na(data)) 
```

As we can see, there are no missing values in our dataset.

```{r}
#Checking outliers:
par(mfrow=c(1,3)) 
boxplot(data$injuries, col="lightgoldenrod", main="Deaths by injuries")
boxplot(data$zymotic, col="mistyrose", main="Deaths by zymotic disease")
boxplot(data$other, col="powderblue", main="Deaths by other causes")
```

Let's recall what we can appreciate in the plots above:

- The majority of deaths by injury are compressed by 0 and 130, approximately. Additionally, there are some exceptional periods where deaths by injury amounted up to 300.

- The majority of deaths by zymotic disease are compressed between 50 and 800 approximately. However, we can see a long tail of cases up to 1700, with some additional outliers of 2100 and 2800.

- The majority of deaths by other alternative causes are compressed between 25 and 75 approximately. We can also appreciate a long tail up to 175 and some outliers of 300 and 350.

To sum up, it seems for now that most of the deaths included in our dataset were caused by zymotic diseases.

## Feature Engineering 

```{r}
data[17,1] <- "Aug 1855" #We modify this entry as in the original dataset it appeared as Aug_1855.
```

As the periods in our dataset are prior to January 1900, R cannot handle them as proper dates. For this reason, we are going to treat time as a numerical variable taking integer value from 0 to 23 in chronological order. This table of equivalences may be useful for the interpretation of the results in this report.

```{r}
#Creating a numeric variable to account for time period:
L <- nrow(data)
time_period <- seq(0,(L-1)) #Changing from 1 to L
data$time_period <- time_period
data <- as.data.frame(data)

time_vars <- select(data, month, time_period)
time_vars %>% 
  gt() %>%
    tab_header(title = md("**Time periods**"), subtitle = md("Equivalence between both variables"))
```
.  

```{r}
#Adding new variables to the dataset:
deaths <- vector() 
for(i in 1:L){
  deaths[i] <- data$zymotic[i] + data$injuries[i] + data$other[i]
}
data$total_deaths <- deaths #Agreggated deaths (all causes) per month

cum_deaths <- vector()
cum_deaths <- cumsum(deaths)
data$cum_deaths <- cum_deaths #Cumulative deaths over time
```

In order to have a better understanding of the data presented, we created a new variable which contains the sum of the different death causes (zymotic, injuries and other) combined. Each entry of this variable will be seggregated by period/month of the year. The name of this new variable is "total_deaths".

In addition, we created another new variable that aims to explain the cumulative deaths over time. Thus, using the new variable of deaths generated just before, we create a new one in order to explain how this aggregated deaths perform over time. The name of this variable is "cum_deaths". 

```{r}
deaths_evol <- select(data, month, total_deaths, time_period)
ordered_deaths <- deaths_evol %>% 
  arrange(desc(deaths_evol$total_deaths)) #Months arranged from higher to lower number of deaths

#Visualizing in a colorful data table the changes performed:
table_deaths <- data.frame(
  Month = ordered_deaths$month,
  TotalDeaths = ordered_deaths$total_deaths,
  TimePeriod = ordered_deaths$time_period)

formattable(table_deaths, list(
  Month = color_tile("lightblue", "lightpink4"),
  TotalDeaths = color_bar("grey")))
```

The above table shows our dataset but sorted by the number of total deaths in descending order. We can observe that
the time period with the highest number of deaths is January 1855, the 10th time period in chronological order. Not surprisingly, April 1854, the first period in our dataset, is the one with the fewest deaths.

## Seasonality Analysis

As we are working with time series, we must first check for stationarity in the data. Depending on whether the variables present a seasonal component or not, the analysis techniques and their interpretations will differ.

```{r}
par(mfrow=c(2,2)) 
plot(data$avg_size_army, type="line", main="Average Size Army", xlab="Time", ylab="Average Size Army", col="azure4")
plot(data$zymotic, type="line", main="Zymotic Diseases", xlab="Time", ylab="Zymotic Diseases", col="firebrick4")
plot(data$injuries, type="line", main="Injuries", xlab="Time", ylab="Injuries", col="gold4")
plot(data$other, type="line", main="Other Causes", xlab="Time", ylab="Other Causes", col="cyan4")
```

After visually inspecting the four main series, we can state that no seasonal components are observable in any of them. Hence, we will assume that neither of our variables have seasonal effects.

*****
# Data Visualization
*****

We are going to explore the dataset in a visual way to see the impacts some variables had on some others:

```{r}
#How did the different type of deaths measured in absolute values evolve in time?
data_plot_1 <- data.frame(
  Time=data$time_period, 
  Zymotic=data$zymotic, 
  Injuries=data$injuries,
  Other=data$other)
dygraph(data_plot_1, main="Death causes (absolute values)")
```

Zymotic disease, with the exception of preriod 17, is the disease that generated more deaths per period. In period 9, the number of deaths for zymotic disease were more than 5 times higher than injuries and other disease.

```{r}
#How did the different type of deaths measured in rate values evolve in time?
data_plot_2 <- data.frame(
  Time=data$time_period,
  Zymotic=data$zymotic_rate, 
  Injuries=data$injuries_rate,
  Other=data$other_rate)
dygraph(data_plot_2, main="Death causes (rates)")
```

As expected, the results achieved are very similar than in the graph above.

```{r}
#How did the size of the army evolve in time?
data_plot_3 <- data.frame(
  Time=data$time_period,
  Army=data$avg_size_army)
dygraph(data_plot_3, main="Average size of the army")
```

Despite the high amount of deaths that we observed in the graphs above, especially in period 9, the size of the army seems not to be that much affected. This can be caused by some hidden information not captured by the variables given in the dataset, such as for example the incorporation of new soldiers along the war to substitute their dead counterparts.

```{r}
#How did the accumulated number of deaths evolve in time?
data_plot_4 <- data.frame(
  Time=data$time_period,
  Accumulated_deaths=cum_deaths) 
dygraph(data_plot_4, main="Accumulated number of deaths")
```

The number of deaths keeps increasing as the war takes place. Especially from periods 9 to 15 we can appreciate an accelerated increase in the accumulated deaths of the army. However, in the last period, we can see a stabilization of sich number, meaning that the war is arriving at its end.

Let's analyze now the number of deaths by period and disease in absolute values:

```{r}
#We display the data that is more suitable for the graphs:
data=data[1:12,]
data1=data.frame(t(data))
data2=data1[3:5,]
colnames(data2)=month.abb
data2$group=row.names(data2)
data3=melt(data2,id="group")
data3$value=as.numeric(data3$value)
head(data3)
```
```{r}
#Bar graph:
ggplot(data=data3,aes(x=variable,y=value,fill=group))+
        geom_bar(stat="identity")+
        scale_fill_brewer(palette="RdBu")+xlab("")+ylab("")
```
```{r}
#Coordenades polar graph:
ggplot(data=data3,aes(x=variable,y=value,fill=group))+
        geom_bar(stat="identity")+
        coord_polar()+
        scale_fill_brewer(palette="BuPu")+xlab("")+ylab("")
```
```{r}
#Heatmap graph:
ggplot(data=data3,aes(x=variable,y=group,fill=value))+
        geom_tile(colour="black",size=0.1)+
        scale_fill_gradientn(colours=c("white","deepskyblue3"))+
        coord_polar()+xlab("")+ylab("")
```

All three last graph display the same data from different angles. Taking into consideration that the first data we have is from April 1854 and the last one from March 1855, this completes a whole year from different period. Thus, from the data displayed in the graphs, we can identify that the number of deaths experience an exponential increment from April (the war is starting) until October, where it reaches its maximum. From there, is decreases until there is barely any death at the end of March, meaning as well that the war is arriving to an end.

*****
# Regression Analysis
*****

The aim on this section is to show how the variables correlate to each other. Morever, we explore through regressions whether some variables have a significant impact on other variables.

```{r}
#Some of the regression models performed below will not appear in the final HTML report as some of them have regressors that are far from being significant and residuals that are not normally distributed. Hence, below we present different regression models we thought were worth running and investigating even though some yield non satisfactory results.
```

```{r}
#Average Size Army vs Zymotic Diseases:
cor(data$avg_size_army, data$zymotic)  
reg1 <- lm(avg_size_army ~ zymotic, data=data)
coeftest(reg1, NeweyWest(reg1))

#Residuals:
resid1 <- as.numeric(reg1$residuals) 
adjusted.values1 <- fitted(reg1) 
plot(adjusted.values1, resid1, main="Residual plot Model 1", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid1, col="tomato", main="Quantile-Comparison Plot Model 1")
qqline(resid1, lwd=2, lty=3)

shapiro.test(resid1)

#The correlation for this model is 0.55, meaning that the correlation will be positive and intens (its nearest point is 1). In plain text, correlation is explaining that if the intercept variable (avg_size_army) is growing at 1%, zymotic diseases will grow at 0.55%.

#After calculating the correlation and the simple regression of these two variable, NeweyWest test is applied. This test provides an estimate of the covariance matrix of the parameters of a regression-type model when this model is applied in situations where the standard assumptions of regression analysis do not apply. Summarizing, NeweyWest test wants not to ignore how yesterday's values may impact today's values.

#From the t test of coefficients regression, taking into consideration the NeweyWest method applied, we can see that the average size of the army is positively related to zymotic diseases. For each soldier added in the army, zymotic diseases increased by 4.16 (soldiers killed?). There is a certain level of signification between the two variables.

#The residual plot of this model let us see, in a graphical way, the distributions of those residuals, which are in this case normal. On the other hand, from the quartile-comparison plot model we can see a normal distribution, although with some outliers in the ends.

#Finally, Shapiro-Wilk test performed is understood as a test of normality in which the null-hypothesis of this test is that the population is normally distributed. Thus, if the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed. On the other hand, if the p value is greater than the chosen alpha level, then the null hypothesis that the data came from a normally distributed population can not be rejected. In our case, as we assume the significance level to be at least 0.05 and our significance is 0.001, we will retain the null hypothesis of population normality as p > 0.05.

#Like most statistical significance tests, if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis (i.e., although there may be some statistically significant effect, it may be too small to be of any practical significance); thus, additional investigation of the effect size is typically advisable, e.g., a Q–Q plot in this case.
```
```{r}
#Average Size Army vs Deaths by Injuries:
cor(data$avg_size_army, data$injuries)
reg2 <- lm(avg_size_army ~ injuries, data=data)
coeftest(reg2, NeweyWest(reg2))

#Residuals:
resid2 <- as.numeric(reg2$residuals) 
adjusted.values2 <- fitted(reg2) 
plot(adjusted.values2, resid2, main="Residual plot Model 2", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid2, col="tomato", main="Quantile-Comparison Plot Model 2")
qqline(resid2, lwd=2, lty=3)

shapiro.test(resid2)

#The correlation for this model is 0.34, meaning that the correlation will be positive but not so intens as its nearest point is 0. Thus, correlation is explaining that if the intercept variable (avg_size_army) is growing at 1%, deaths caused by injuries will grow at 0.34%.

#After calculating the correlation and the simple regression of these two variable, NeweyWest test is applied as before. NeweyWest test wants not to ignore how yesterday's values may impact today's values.

#From the t test of coefficients regression, taking into consideration the NeweyWest method applied, we can see that the average size of the army is positively, however slightly related to deaths by injuries. For each soldier added in the army, deaths by injuries increased by 26.83 (soldiers killed?). There is a certain level of signification between the two variables.

#The residual plot of this model let us see, in a graphical way, the distributions of those residuals, which appears not to be normal. On the other hand, from the quartile-comparison plot model we can see a normal distribution, although with some outliers in the ends.

#Finally, Shapiro-Wilk test performed is understood as a test of normality. In our case, as we assume the significance level to be at least 0.05 and our significance is 0.00035, we will retain the null hypothesis of population normality as p > 0.05.
```
```{r, include = FALSE}
#Average Size Army vs Other Causes:
cor(data$avg_size_army, data$other)
reg3 <- lm(avg_size_army ~ other, data=data)
coeftest(reg3, NeweyWest(reg3))

#Residuals:
resid3 <- as.numeric(reg3$residuals) 
adjusted.values3 <- fitted(reg3) 
plot(adjusted.values3, resid3, main="Residual plot Model 3", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid3, col="tomato", main="Quantile-Comparison Plot Model 3")
qqline(resid3, lwd=2, lty=3)

shapiro.test(resid3)
```

Baixa correlació

```{r, include = FALSE}
#Average Size Army vs. all deaths (segregated):
#falta cor
reg4 <- lm(avg_size_army ~ data$zymotic+data$injuries+data$other, data=data)
coeftest(reg4, NeweyWest(reg4))

#Residuals:
resid4 <- as.numeric(reg4$residuals) 
adjusted.values4 <- fitted(reg4) 
plot(adjusted.values4, resid4, main="Residual plot Model 4", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid4, col="tomato", main="Quantile-Comparison Plot Model 4")
qqline(resid4, lwd=2, lty=3)

shapiro.test(resid4)
```

Baixa correlació en totes

```{r, include = FALSE}
#Average Size Army vs. Total Deaths:
cor(data$avg_size_army, data$total_deaths) 
reg5 <- lm(avg_size_army ~ total_deaths, data=data)
coeftest(reg5, NeweyWest(reg5))

#Residuals:
resid5 <- as.numeric(reg5$residuals) 
adjusted.values5 <- fitted(reg5) 
plot(adjusted.values5, resid5, main="Residual plot Model 5", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid5, col="tomato", main="Quantile-Comparison Plot Model 5")
qqline(resid5, lwd=2, lty=3)

shapiro.test(resid5)
```

Baixa correlació

```{r, include = FALSE}
#Zymotic Diseases vs. Time Period:
cor(data$zymotic, data$time_period)
reg6 <- lm(zymotic ~ time_period, data=data)
coeftest(reg6, NeweyWest(reg6))

#Residuals:
resid6 <- as.numeric(reg6$residuals) 
adjusted.values6 <- fitted(reg6) 
plot(adjusted.values6, resid6, main="Residual plot Model 6", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid6, col="tomato", main="Quantile-Comparison Plot Model 6")
qqline(resid6, lwd=2, lty=3)

shapiro.test(resid6)
```

Alta correlació

```{r, include = FALSE}
#Deaths by Injuries vs. Time Period:
cor(data$injuries, data$time_period)
reg7 <- lm(injuries ~ time_period, data=data)
coeftest(reg7, NeweyWest(reg7))

#Residuals:
resid7 <- as.numeric(reg7$residuals) 
adjusted.values7 <- fitted(reg7) 
plot(adjusted.values7, resid7, main="Residual plot Model 7", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid7, col="tomato", main="Quantile-Comparison Plot Model 7")
qqline(resid7, lwd=2, lty=3)

shapiro.test(resid7)
```

Baixa correlació

```{r, include = FALSE}
#Other Causes vs. Time Period:
cor(data$other, data$time_period)
reg8 <- lm(other ~ time_period, data=data)
coeftest(reg8, NeweyWest(reg8))

#Residuals:
resid8 <- as.numeric(reg8$residuals) 
adjusted.values8 <- fitted(reg8) 
plot(adjusted.values8, resid8, main="Residual plot Model 8", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid8, col="tomato", main="Quantile-Comparison Plot Model 8")
qqline(resid8, lwd=2, lty=3)

shapiro.test(resid8)
```

Alta correlació

```{r, include = FALSE}
#Total Deaths vs. Time Period
#cor?
reg9 <- lm(total_deaths ~ time_period, data=data)
coeftest(reg9, NeweyWest(reg9))

#Residuals:
resid9 <- as.numeric(reg9$residuals) 
adjusted.values9 <- fitted(reg9) 
plot(adjusted.values9, resid9, main="Residual plot Model 9", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid9, col="tomato", main="Quantile-Comparison Plot Model 9")
qqline(resid9, lwd=2, lty=3)

shapiro.test(resid9)
```

Alta correlació


```{r, include = FALSE}
#For the following model we are interested in total deaths in terms of previous values of the same variable. That is, we want to lag the variable and regress total death on its lags to look for significance. The first step, though, is assess for stationarity of the series.

data_plot_5 <- data.frame(
  Time=data$time_period,
  Total=data$total_deaths)
dygraph(data_plot_5, main="Total deaths (all causes aggregated)")

#The series depicted does not seem very stationary. Nonetheless, we counduct a Dickey-Fuller test.
adf.test(data$total_deaths)
#As we cannot reject the null hypotheses of non-stationarity, we will differenciate the data in order to obtain stationarity and be able to run an autoregressive model:

total_deaths_diff <- diff(data$total_deaths)
adf.test(total_deaths_diff)
#By looking at the p-value returned by the test, we can state that the differenced series is not stationary at a 0.05 significance. We differenciate again:

total_deaths_diff2 <- diff(total_deaths_diff)
adf.test(total_deaths_diff2)
#Now that we have finally obtained a stationary series, we can perform an autoregressive model.

#Checking how many lags we should include:
acf(total_deaths_diff2, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Autocorrelation function total deaths") 
pacf(total_deaths_diff2, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Parcial autocorrelation function total deaths") 

#None of the lags are significant so, even though after differencing our data twice, it does not make sense to construct an ARIMA model. As the data differenced only once is almost stationary, we can check if it would make mmore sense to construct an ARIMA model in such case:
acf(total_deaths_diff, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Autocorrelation function total deaths") 
pacf(total_deaths_diff, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Parcial autocorrelation function total deaths") 
#Again, no lags are significant and thus we will not be able to construct a good autoregressive model in this case.



#Drama: amb la data no estacionària no es pot aplicar el model autoregressiu però un cop la diferencio i es converteix en estacionària (total_deaths_diff2) o quasi-estacionària (total_death_diff) no hi ha cap lag (potencials regressors) que semblin ser significatius. Per tant, a la regressió tampoc ho seran --> No afegim aquests dos models? Jo deixaria el raonament aquest explicant why not (el que ja he explicat més amunt) --> Esborrem tot lo de sota?

tra_death <- data.frame(y=data$total_deaths[2:L], lag1=data$total_deaths[1:(L-1)]) 
cor(tra_death$y, tra_death$lag1)
reg10 <- lm(y ~ lag1, data=tra_death)
summary(reg10)
coeftest(reg10, NeweyWest(reg10))

resid10 <- as.numeric(reg10$residuals)
qqnorm(resid10, col="tomato", main="Model 10: Total Deaths vs. Total Deaths Lagged")
qqline(resid10, lwd=2, lty=3)

#Una altra manera de crear models autoregressius:
reg11 <- arima(data$total_deaths, c(1,0,0))
coeftest(reg11)

resid11 <- as.numeric(reg11$residuals)
qqnorm(resid11, col="tomato", main="Model 11: Total Deaths vs. Total Deaths Lagged AR(1)") 
qqline(resid11, lwd=2, lty=3)

#Veiem que tant una regressió lineal calculada amb la funció lm() dóna el mateix model que quan apliquem un model AR(1) aplicant la funció arima()
```

*****
# Predictions
*****

Triar model!

```{r}
#Accuracy de reg1 per fer prediccions (he provat de fer la del número de morts basant-me només en un lag d'aquesta mateixa variable per provar de fer-ne alguna)
train_data <- subset(data, time_period<=round(0.7*L)) #70% training data
test_data <- subset(data, time_period>round(0.7*L)) #30% test/validation data
training1 <- data.frame(y=train_data$total_deaths, lag1=train_data$total_deaths) 
model1 <- lm(y ~ lag1, data=training1)
y.hat1 <- predict(model1, data=test_data) 
error1 <- abs(y.hat1 - test_data$total_deaths)
rmse1 <- sqrt((sum(error1)**2)/nrow(test_data)) 
```
```{r}
# Ho hem de fer amb un model que funcioni però... Quina seria la probabilitat de X si Y és això i Z és això altre?
#pred<-predict(reg1, data.frame(Y="1",Z=90),type = "response")
#pred
```
```{r}
# Calculem la bondat de l'ajust: <- s'ha de tocar la data
#hoslem.test(X,fitted(modelX))
```
```{r}
# Dibuixem la corba ROC: <- s'ha de tocar la data
#prob_low=predict(logit_model_2, datA3, type="response")
#r=roc(BW_RE,prob_low, data=datA3)
```

*****
# Conclusions
*****

*****
# Web References
*****
Useful links to learn more about Florence Nightingale and her legacy:

https://theconversation.com/the-healing-power-of-data-florence-nightingales-true-legacy-134649

https://www.history.com/topics/womens-history/florence-nightingale-1

https://mujeresconciencia.com/2017/08/22/florence-nightingale-mucho-mas-la-dama-la-lampara/