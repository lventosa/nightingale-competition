---
title: "Florence Nightingale Competition 2020 - RLadies Spain"
author: "Authors: Laura Ventosa and Esther Manzano"
date: "July 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---
```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
``` 

*****
# Introduction
*****

**Florence Nightingale** (1820-1910), known as *The Lady With the Lamp*, was a British nurse, social reformer and statistician best known as the founder of modern nursing. Her experiences as a nurse during the Crimean War were foundational in her views about sanitation. She established St. Thomasâ€™ Hospital and the Nightingale Training School for Nurses in 1860. Her efforts to reform healthcare greatly influenced the quality of care in the 19 and 20 centuries.

In 1854, she and 38 other nurses trained by herself traveled to the Crimean peninsula to treat the wounded British soldiers in the Crimean War (October 1853 - March 1856). After their arrival, the nurses discovered that the treatments the injured soldiers were receiving were inaccurate and that the overall hygenic conditions and medical supplies were insufficient. Florence kept a record of the monthly deaths by injuries, zymotic diseases and other causes among the soldiers between April 1854 and March 1856. These data were going to be later used for her rose diagrams.

One of her most important discoveries was the fact that "deaths caused by diseases were more than seven times the number of deaths due to combat", because of unsanitary hospital conditions, as mentioned. However, knowing numbers alone have limited persuasive powers, Nightingale used her skills in statistical communication to convince the British parliament of the need to act. She avoided the dry tables used by most statisticians of the time, and instead devised a novel graph to illustrate the impact of hospital and nursing practice reform on army mortality rates.


*****
# Data Exploration
*****

We check the different types of data on our dataset. We want to understand the nature of our variables (i.e. the proportion of numerical and categorical variables) to be able to develope a better model afterwards. Checks on missing values and outliers will be performed in addition.

```{r}
rm(list=ls()) 

#Libraries used across the whole project:
library(readxl)
library(dplyr)
library(tidyverse)
library(gt)
library(dygraphs)
library(formattable)
library(xts) 
library(lmtest)
library(tseries)
library(sandwich)
library(forecast)
library(ResourceSelection)
library(pROC)
library(reshape2)
```
```{r}
#Importing dataset:
data <- read_excel("../nightingale-competition/datos_florence.xlsx", skip=1)
```

## Basic Information 

The following table shows the first rows of our dataset, including the names of the variables we have worked with.

```{r}
#Basic information:
head(data)
```
```{r}
#Dimensions of the dataset:
dim(data)
```

We can observe that the dimensions of our original dataset are 24x8. That is, 24 rows and 8 columns.

```{r}
#Variables:
sapply(data, class)
```
```{r}
summary(data)
```

Nightingale's original dataset consists of the following variables:

- Month (1): Character variable stating the month and year (period).

- Average size of army (2): Numeric variable that informs the reader about the average size of the army in such period of time.

- Zymotic diseases (3): Numeric variable which states the number of soldiers who died in such period of time for zymotic disease causes in absolute values.

- Wounds & injuries (4): Numeric variable which states the number of soldiers who died in such period of time for wounds and injuries causes in absolute values.

- All other causes (5): Numeric variable which states the number of soldiers who died in such period of time for any other causes in absolute values.

- Zymotic diseases (6): Numeric variable which states the number of soldiers who died in such period of time for zymotic disease causes in rate values.

- Wounds & injuries (7): Numeric variable which states the number of soldiers who died in such period of time for wounds and injuries causes in rate values.

- All other causes (8): Numeric variable which states the number of soldiers who died in such period of time for any other causes in rate values.

In order to make the data more readeable and understandable, we will make some changes in the variables as well as some variable additions.

```{r}
#Making everything more readable and column names more manageable:
colnames(data)[1] <- "month"
colnames(data)[2] <- "avg_size_army"
colnames(data)[3] <- "zymotic"
colnames(data)[4] <- "injuries"
colnames(data)[5] <- "other"
colnames(data)[6] <- "zymotic_rate"
colnames(data)[7] <- "injuries_rate"
colnames(data)[8] <- "other_rate"
```

## Missing Values and Outliers

We are interested in knowing whether we have missing observations before start treating our data.

```{r}
#Checking missing values:
colSums(is.na(data)) 
```

As we can see, there are no missing values in our dataset.

```{r}
#Checking outliers:
par(mfrow=c(1,3)) 
boxplot(data$injuries, col="lightgoldenrod", main="Deaths by injuries")
boxplot(data$zymotic, col="mistyrose", main="Deaths by zymotic disease")
boxplot(data$other, col="powderblue", main="Deaths by other causes")
```

Let's recall what we can appreciate in the plots above:

- The majority of deaths by injury are compressed by 0 and 130, approximately. Additionally, there are some exceptional periods where deaths by injury amounted up to 300.

- The majority of deaths by zymotic disease are compressed between 50 and 800 approximately. However, we can see a long tail of cases up to 1700, with some additional outliers of 2100 and 2800.

- The majority of deaths by other alternative causes are compressed between 25 and 75 approximately. We can also appreciate a long tail up to 175 and some outliers of 300 and 350.

To sum up, it seems for now that most of the deaths included in our dataset were caused by zymotic diseases.

## Feature Engineering 

```{r}
data[17,1] <- "Aug 1855" #We modify this entry as in the original dataset it appeared as Aug_1855.
```

As the periods in our dataset are prior to January 1900, R cannot handle them as proper dates. For this reason, we are going to treat time as a numerical variable taking integer value from 0 to 23 in chronological order. This table of equivalences may be useful for the interpretation of the results in this report.

```{r}
#Creating a numeric variable to account for time period:
L <- nrow(data)
time_period <- seq(0,(L-1)) #Changing from 1 to L
data$time_period <- time_period
data <- as.data.frame(data)

time_vars <- select(data, month, time_period)
time_vars %>% 
  gt() %>%
    tab_header(title = md("**Time periods**"), subtitle = md("Equivalence between both variables"))
```
.  

```{r}
#Adding new variables to the dataset:
deaths <- vector() 
for(i in 1:L){
  deaths[i] <- data$zymotic[i] + data$injuries[i] + data$other[i]
}
data$total_deaths <- deaths #Agreggated deaths (all causes) per month

cum_deaths <- vector()
cum_deaths <- cumsum(deaths)
data$cum_deaths <- cum_deaths #Cumulative deaths over time
```

In order to have a better understanding of the data presented, we created a new variable which contains the sum of the different death causes (zymotic, injuries and other) combined. Each entry of this variable will be seggregated by period/month of the year. The name of this new variable is "total_deaths".

In addition, we created another new variable that aims to explain the cumulative deaths over time. Thus, using the new variable of deaths generated just before, we create a new one in order to explain how this aggregated deaths perform over time. The name of this variable is "cum_deaths". 

```{r}
deaths_evol <- select(data, month, total_deaths, time_period)
ordered_deaths <- deaths_evol %>% 
  arrange(desc(deaths_evol$total_deaths)) #Months arranged from higher to lower number of deaths

#Visualizing in a colorful data table the changes performed:
table_deaths <- data.frame(
  Month = ordered_deaths$month,
  TotalDeaths = ordered_deaths$total_deaths,
  TimePeriod = ordered_deaths$time_period)

formattable(table_deaths, list(
  Month = color_tile("lightblue", "lightpink4"),
  TotalDeaths = color_bar("grey")))
```

The above table shows our dataset but sorted by the number of total deaths in descending order. We can observe that the time period with the highest number of deaths is January 1855, the 10th time period in chronological order. Not surprisingly, April 1854, the first period in our dataset, is the one with the fewest deaths.

## Seasonality Analysis

As we are working with time series, we must first check for stationarity in the data. Depending on whether the variables present a seasonal component or not, the analysis techniques and their interpretations will differ.

```{r}
par(mfrow=c(2,2)) 
plot(data$avg_size_army, type="line", main="Average Size Army", xlab="Time", ylab="Average Size Army", col="azure4")
plot(data$zymotic, type="line", main="Zymotic Diseases", xlab="Time", ylab="Zymotic Diseases", col="firebrick4")
plot(data$injuries, type="line", main="Injuries", xlab="Time", ylab="Injuries", col="gold4")
plot(data$other, type="line", main="Other Causes", xlab="Time", ylab="Other Causes", col="cyan4")
```

After visually inspecting the four main series, we can state that no seasonal components are observable in any of them. Hence, we will assume that neither of our variables have seasonal effects.

*****
# Data Visualization
*****

We are going to explore the dataset in a visual way to see the impacts some variables had on some others:

```{r}
#How did the different type of deaths measured in absolute values evolve in time?
data_plot_1 <- data.frame(
  Time=data$time_period, 
  Zymotic=data$zymotic, 
  Injuries=data$injuries,
  Other=data$other)
dygraph(data_plot_1, main="Death causes (absolute values)")
```

Zymotic disease, with the exception of preriod 17, is the disease that generated more deaths per period. In period 9, the number of deaths for zymotic disease were more than 5 times higher than injuries and other disease.

```{r}
#How did the different type of deaths measured in rate values evolve in time?
data_plot_2 <- data.frame(
  Time=data$time_period,
  Zymotic=data$zymotic_rate, 
  Injuries=data$injuries_rate,
  Other=data$other_rate)
dygraph(data_plot_2, main="Death causes (rates)")
```

As expected, the results achieved are very similar than in the graph above.

```{r}
#How did the size of the army evolve in time?
data_plot_3 <- data.frame(
  Time=data$time_period,
  Army=data$avg_size_army)
dygraph(data_plot_3, main="Average size of the army")
```

Despite the high amount of deaths that we observed in the graphs above, especially in period 9, the size of the army seems not to be that much affected. This can be caused by some hidden information not captured by the variables given in the dataset, such as for example the incorporation of new soldiers along the war to substitute their dead counterparts.

```{r}
#How did the accumulated number of deaths evolve in time?
data_plot_4 <- data.frame(
  Time=data$time_period,
  Accumulated_deaths=cum_deaths) 
dygraph(data_plot_4, main="Accumulated number of deaths")
```

The number of deaths keeps increasing as the war takes place. Especially from periods 9 to 15 we can appreciate an accelerated increase in the accumulated deaths of the army. However, in the last period, we can see a stabilization of sich number, meaning that the war is arriving at its end.

Let's analyze now the number of deaths by period and disease in absolute values:

```{r}
#We display the data that is more suitable for the graphs:
data_1year=data[1:24,]
#data_2year=data[13:24,]

data1=data.frame(t(data_1year))
#data1.1=data.frame(t(data_2year))
data2=data1[3:5,]
#data2.1=data1.1[3:5,]

colnames(data2)=c("Apr 1854","May 1854","Jun 1854","Jul 1854","Aug 1854","Sep 1854","Oct 1854","Nov 1854","Dec 1854","Jan 1855","Feb 1855","Mar 1855","Apr 1855","May 1855","Jun 1855","Jul 1855","Aug 1855","Sep 1855","Oct 1855","Nov 1855","Dec 1855","Jan 1856","Feb 1856","Mar 1856")
#colnames(data2)=month.abb
#data2$group=row.names(data2)
#colnames(data2.1)=month.abb
#data2.1$group=row.names(data2.1)


#data3=melt(data2,id="data2")
#data3$value=as.numeric(data3$value)
#data3=melt(data2)
#data3.1$value=as.numeric(data3.1$value)

head(data2)
#head(data3.1)

df2 <- data.frame(t(data2[-1]))

head(df2)
```
```{r}
#Bar graph:

data2 <- data.frame(
  name=data$zymotic, data$injuries data$other,
  value=c("Apr 1854","May 1854","Jun 1854","Jul 1854","Aug 1854","Sep 1854","Oct 1854","Nov 1854","Dec 1854","Jan 1855","Feb 1855","Mar 1855","Apr 1855","May 1855","Jun 1855","Jul 1855","Aug 1855","Sep 1855","Oct 1855","Nov 1855","Dec 1855","Jan 1856","Feb 1856","Mar 1856")
)

# barplot
barplot( height=data2$value, names=data2$name , col="brown"   )


values=c("Apr 1854","May 1854","Jun 1854","Jul 1854","Aug 1854","Sep 1854","Oct 1854","Nov 1854","Dec 1854","Jan 1855","Feb 1855","Mar 1855","Apr 1855","May 1855","Jun 1855","Jul 1855","Aug 1855","Sep 1855","Oct 1855","Nov 1855","Dec 1855","Jan 1856","Feb 1856","Mar 1856")

hola=c("zymotic", "injuries", "other")

g <- ggplot(data2, aes(x=values))
# Number of cars in each class:
g + geom_bar()

subset <- data.frame(data2)
#barplot(subset, main = "hola", xlab="number fo gears", legend = c("zymotic", "injuries", "other"))

values=c("Apr 1854","May 1854","Jun 1854","Jul 1854","Aug 1854","Sep 1854","Oct 1854","Nov 1854","Dec 1854","Jan 1855","Feb 1855","Mar 1855","Apr 1855","May 1855","Jun 1855","Jul 1855","Aug 1855","Sep 1855","Oct 1855","Nov 1855","Dec 1855","Jan 1856","Feb 1856","Mar 1856")

#hola=c("zymotic", "injuries", "other")

ggplot(data=df2,aes(x=values, y=hola))
        geom_bar(stat="identity")+
        scale_fill_brewer(palette="RdBu")+xlab("")+ylab("")
```
```{r}
#Coordenades polar graph:
#ggplot(data=data3,aes(x=variable,y=value,fill=group))+
        #geom_bar(stat="identity")+
        #coord_polar()+
        #scale_fill_brewer(palette="BuPu")+xlab("")+ylab("")
```
```{r}
#Heatmap graph:
#ggplot(data=data3,aes(x=variable,y=group,fill=value))+
        #geom_tile(colour="black",size=0.1)+
        #scale_fill_gradientn(colours=c("white","deepskyblue3"))+
        #coord_polar()+xlab("")+ylab("")
```

All three last graph display the same data from different angles. Taking into consideration that the first data we have is from April 1854 and the last one from March 1855, this completes a whole year from different periods. Thus, from the data displayed in the graphs, we can identify that the number of deaths experience an exponential increment from April (the war is starting) until October, where it reaches its maximum. From there, is decreases until there is barely any death at the end of March, meaning as well that the war is arriving to an end.

*****
# Regression Analysis
*****

The aim of this section is to show how the variables correlate to each other. Morever, we explore through regressions whether some variables have a significant impact on other variables. 

As the variables in our data set represent time series, some degree of autocorrelation within each variable is expected. For this reason, and to be able to better asses significance of the variables when used in regression models, standard errors are computed using the Newey West method, which accounts for this autocorrelation. 

We have also analyzed the normality of the residuals after fitting the model and discarded those models with non-normal residuals. Normality is assed graphically with a QQPlot and running the Shapiro-Wilk test (null hypothesis: normality). For the interpretation of this test and for when assessing statistical significance of coefficients, we set a 0.05 (5%) significance level. 

This knitted report only includes the regression models that, after being fitting, had normally distributed residuals.

```{r}
#Some of the regression models performed below will not appear in the final HTML report as some of them have regressors that are far from being significant and residuals that are not normally distributed. Hence, below we present different regression models we thought were worth running and investigating even though some yield non satisfactory results.
``` 

```{r, include = FALSE}
#Average Size Army vs Zymotic Diseases:
cor(data$avg_size_army, data$zymotic)  
reg1 <- lm(avg_size_army ~ zymotic, data=data)
coeftest(reg1, NeweyWest(reg1))

#Residuals:
resid1 <- as.numeric(reg1$residuals) 
adjusted.values1 <- fitted(reg1) 
plot(adjusted.values1, resid1, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid1, col="tomato", main="Quantile-Comparison Plot")
qqline(resid1, lwd=2, lty=3)

shapiro.test(resid1)

#The correlation coefficient for this model is 0.55, meaning that the amount of deaths caused by zymotic diseases and the average size of the army will be positively correlated.

#By looking at the table of coefficients, we can state that only the intercept is statistically significant and that the amount of deaths by zymotic diseases does not have a significant effect on the average size of the army.

#By the Shapiro test, we reject the null hypothesis of normality in the residuals and hence conclude that this model is not very trustworthy. 
```

## Average size army vs Deaths by injuries

```{r}
cor(data$avg_size_army, data$injuries)
reg2 <- lm(avg_size_army ~ injuries, data=data)
coeftest(reg2, NeweyWest(reg2))

#Residuals:
resid2 <- as.numeric(reg2$residuals) 
adjusted.values2 <- fitted(reg2) 
plot(adjusted.values2, resid2, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid2, col="tomato", main="Quantile-Comparison Plot")
qqline(resid2, lwd=2, lty=3)

shapiro.test(resid2)
```

The correlation coefficient between deaths by injuries and the average size of the army is aprox 0.25, quite low but positive. 

This model is reliable judging by the results of the Shapiro test (we do not reject the null hypotheses of normally distributed residuals). The above plots also show that the residuals do not happen to be normally distributed. Nonetheless, only the intercept is significant. Hence, we conclude by saying that the amount of deaths by injuries does not have a significant effect on the average size of the army.

```{r, include = FALSE}
#Average Size Army vs Other Causes:
cor(data$avg_size_army, data$other)
reg3 <- lm(avg_size_army ~ other, data=data)
coeftest(reg3, NeweyWest(reg3))

#Residuals:
resid3 <- as.numeric(reg3$residuals) 
adjusted.values3 <- fitted(reg3) 
plot(adjusted.values3, resid3, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid3, col="tomato", main="Quantile-Comparison Plot")
qqline(resid3, lwd=2, lty=3)

shapiro.test(resid3)

#Since the correlation coefficient is -0.16, these two variables move in opposite directions. That is, when one variable increases, the other one is expected to decrease.

#Again, only the intercept is statistically significant in this regression model. Thus, deaths by causes other than injuries and wounds and symotic diseases do not appear to have a significant impact on the average size of the army.

#The residuals do not look normally distributed. Moreover, the Shapiro test shows that we must reject the null hypothesis of normality. Consequently, we can state that this regression model is not very reliable.
```

## Average size army vs. All deaths (multilinear regression model)

```{r}
reg4 <- lm(avg_size_army ~ data$zymotic+data$injuries+data$other, data=data)
coeftest(reg4, NeweyWest(reg4))

#Residuals:
resid4 <- as.numeric(reg4$residuals) 
adjusted.values4 <- fitted(reg4) 
plot(adjusted.values4, resid4, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid4, col="tomato", main="Quantile-Comparison Plot")
qqline(resid4, lwd=2, lty=3)

shapiro.test(resid4)
```

Despite the previous results, when we regress the average size of the army against all three causes of death in the dataset, we obtain that the fitted model has normally distributed residuals. 

Nonetheless, neither of the coefficients are significant, allowing us to conclude that neither deaths by injuries, deaths by zymotic diseases nor deaths by other causes appear to have a significant effect on the average size of the army.

This may appear surprising, as we would expect that the more deaths (no matter the cause), the lower the average size of the army. However, we must also bear in mind that the variables in the dataset do not explain the whole story. The dataset does not include information of the new soldiers that month after month joined the army. Having soldiers killed may cause the average size of the army to decrease but it may also caused more men to join the army, increasing its average size. Without information of new soldiers, we cannot know the real impact of the causes of death on the average size of the army.

```{r, include = FALSE}
#Average Size Army vs. Total Deaths:
cor(data$avg_size_army, data$total_deaths) 
reg5 <- lm(avg_size_army ~ total_deaths, data=data)
coeftest(reg5, NeweyWest(reg5))

#Residuals:
resid5 <- as.numeric(reg5$residuals) 
adjusted.values5 <- fitted(reg5) 
plot(adjusted.values5, resid5, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid5, col="tomato", main="Quantile-Comparison Plot")
qqline(resid5, lwd=2, lty=3)

shapiro.test(resid5)

#We wanted to compare the results of the previous regression model with the results of regressing the average size of the army against total deaths (aggregated in the new variable we created).

#The correlation between total deaths and the average size of the army is negative and not very strong.

#As for the regression coefficients, only the intercept is statistically significant and thus, total deaths do not have a statistically signifcant effect on the average size of the army. Nonetheless, this is not a very reliable model since we reject the null hypotheses of normally distributed residuals.
```
```{r, include = FALSE}
#Zymotic Diseases vs. Time Period:
cor(data$zymotic, data$time_period)
reg6 <- lm(zymotic ~ time_period, data=data)
coeftest(reg6, NeweyWest(reg6))

#Residuals:
resid6 <- as.numeric(reg6$residuals) 
adjusted.values6 <- fitted(reg6) 
plot(adjusted.values6, resid6, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid6, col="tomato", main="Quantile-Comparison Plot")
qqline(resid6, lwd=2, lty=3)

shapiro.test(resid6)

#Negative correlation between deaths by zymotic diseases and time period mean that over time we expect the amount of deaths by zymotic diseases to decrease, which by one of the plots of section 2 this is true from period 9 (January 1855) onwards.

#Only the intercept of the regression model is significant. Hence, the time period does not have a significant effect on the number of deaths by zymotinc diseases. Morover, this model is not very reliable as residuals are not normally distributed.
```
```{r, include = FALSE}
#Deaths by Injuries vs. Time Period:
cor(data$injuries, data$time_period)
reg7 <- lm(injuries ~ time_period, data=data)
coeftest(reg7, NeweyWest(reg7))

#Residuals:
resid7 <- as.numeric(reg7$residuals) 
adjusted.values7 <- fitted(reg7) 
plot(adjusted.values7, resid7, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid7, col="tomato", main="Quantile-Comparison Plot")
qqline(resid7, lwd=2, lty=3)

shapiro.test(resid7)

#Time period and the amount of deaths by injuries are negatively correlated, even though the correlation coefficient i very low and near to 0.

#Regarding the regression model, neither the intercept nor the estimated coefficient for the time period variable are statistically significant. This allows us to conclude that deaths by injuries are not significantly affected by the time period. Moreover, this is not a very trustworthy model as residuals ar not normally distributed, judging by the plots and the Shapiro test.
```
```{r, include = FALSE}
#Other Causes vs. Time Period:
cor(data$other, data$time_period)
reg8 <- lm(other ~ time_period, data=data)
coeftest(reg8, NeweyWest(reg8))

#Residuals:
resid8 <- as.numeric(reg8$residuals) 
adjusted.values8 <- fitted(reg8) 
plot(adjusted.values8, resid8, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid8, col="tomato", main="Quantile-Comparison Plot")
qqline(resid8, lwd=2, lty=3)

shapiro.test(resid8)

#The correlation coefficient is negative, which means that over time, fewer deaths by causes that are not injuries nor zymotic diseases are expected. However, the plots in section 2 tell another story.

#Just as the previous model, none of the coefficients estimated are statistically significant and the residuals are not normally distributed.
```
```{r, include = FALSE}
#Total Deaths vs. Time Period
cor(data$total_deaths, data$time_period)
reg9 <- lm(total_deaths ~ time_period, data=data)
coeftest(reg9, NeweyWest(reg9))

#Residuals:
resid9 <- as.numeric(reg9$residuals) 
adjusted.values9 <- fitted(reg9) 
plot(adjusted.values9, resid9, main="Residual Plot", ylab="Residual Values", xlab="Adjusted Values", col="lightsalmon3", abline(0, 0), pch=19)

qqnorm(resid9, col="tomato", main="Quantile-Comparison Plot")
qqline(resid9, lwd=2, lty=3)

shapiro.test(resid9)

#A negative correlation coefficient between these two variables, even though it is not close to 1 in absolute value, shows that total deaths are expected to decrease over time. Again, this is not what the plots shown earlier tell.

#Once again, only the intercept is statistically significant. Thus, the amount of deaths (by all causes) cannot be explained by the time period. In addition, non-normally distributed residuals tell us that this is model si not very trustworthy.
```
```{r, include = FALSE}
#For the following model we are interested in total deaths in terms of previous values of the same variable. That is, we want to lag the variable and regress total death on its lags to look for significance. The first step, though, is assess for stationarity of the series.

data_plot_5 <- data.frame(
  Time=data$time_period,
  Total=data$total_deaths)
dygraph(data_plot_5, main="Total deaths (all causes aggregated)")

#The series depicted does not seem very stationary. Nonetheless, we counduct a Dickey-Fuller test.
adf.test(data$total_deaths)
#As we cannot reject the null hypotheses of non-stationarity, we will differenciate the data in order to obtain stationarity and be able to run an autoregressive model:

total_deaths_diff <- diff(data$total_deaths)
adf.test(total_deaths_diff)
#By looking at the p-value returned by the test, we can state that the differenced series is not stationary at a 0.05 significance. We differenciate again:

total_deaths_diff2 <- diff(total_deaths_diff)
adf.test(total_deaths_diff2)
#Now that we have finally obtained a stationary series, we can perform an autoregressive model.

#Checking how many lags we should include:
acf(total_deaths_diff2, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Autocorrelation function total deaths") 
pacf(total_deaths_diff2, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Parcial autocorrelation function total deaths") 

#None of the lags are significant so, even though after differencing our data twice, it does not make sense to construct an ARIMA model. As the data differenced only once is almost stationary, we can check if it would make mmore sense to construct an ARIMA model in such case:
acf(total_deaths_diff, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Autocorrelation function total deaths") 
pacf(total_deaths_diff, ylim=c(-0.2,1), lwd=5, xlim=c(0,15), col="darkorange2", main="Parcial autocorrelation function total deaths") 
#Again, no lags are significant and thus we will not be able to construct a good autoregressive model in this case.



#Drama: amb la data no estacionÃ ria no es pot aplicar el model autoregressiu perÃ² un cop la diferencio i es converteix en estacionÃ ria (total_deaths_diff2) o quasi-estacionÃ ria (total_death_diff) no hi ha cap lag (potencials regressors) que semblin ser significatius. Per tant, a la regressiÃ³ tampoc ho seran --> No afegim aquests dos models? Jo deixaria el raonament aquest explicant why not (el que ja he explicat mÃ©s amunt) --> Esborrem tot lo de sota?

tra_death <- data.frame(y=data$total_deaths[2:L], lag1=data$total_deaths[1:(L-1)]) 
cor(tra_death$y, tra_death$lag1)
reg10 <- lm(y ~ lag1, data=tra_death)
summary(reg10)
coeftest(reg10, NeweyWest(reg10))

resid10 <- as.numeric(reg10$residuals)
qqnorm(resid10, col="tomato", main="Model 10: Total Deaths vs. Total Deaths Lagged")
qqline(resid10, lwd=2, lty=3)

#Una altra manera de crear models autoregressius:
reg11 <- arima(data$total_deaths, c(1,0,0))
coeftest(reg11)

resid11 <- as.numeric(reg11$residuals)
qqnorm(resid11, col="tomato", main="Model 11: Total Deaths vs. Total Deaths Lagged AR(1)") 
qqline(resid11, lwd=2, lty=3)
```

*****
# Predictions
*****

Models 2 and 4 were chosen as the models able to do perform the highest correlations between varaibles. For these reasons, we will be performing the predictions with these two same models as well.

```{r}
#Accuracy of reg2 to perform the predictions (number of deaths with one lag):
train_data <- subset(data, time_period<=round(0.7*L)) #70% training data
test_data <- subset(data, time_period>round(0.7*L)) #30% test/validation data
model1 <- lm(avg_size_army ~ injuries, data=train_data)
y.hat1 <- predict(model1, data=test_data, interval="confidence")
error1 <- abs(y.hat1 - test_data$avg_size_army)
rmse1 <- sqrt((sum(error1)**2)/nrow(test_data)) 
#rmse1 <- sqrt((sum(abs(y.hat1 - test_data$total_deaths))**2)/nrow(test_data))

rmse1
```
```{r}
#Accuracy of reg4 to perform the predictions (number of deaths with one lag):
model2 <- lm(avg_size_army ~ zymotic + injuries + other, data=train_data)
y.hat2 <- predict(model2, data=test_data, interval="confidence")
#rmse2 <- sqrt((sum(abs(y.hat2 - test_data$total_deaths))**2)/nrow(test_data))
error2 <- abs(y.hat2 - test_data$avg_size_army)
rmse2 <- sqrt((sum(error2)**2)/nrow(test_data)) 

rmse2
```
```{r}
#Agreggate predictions:
preds <- data.frame(cbind(y.hat1, y.hat2))

# Regression line + confidence intervals:
preds$group=row.names(preds)
colnames(preds)=month.abb
ggplot(preds, aes(group, y.hat1)) +
  geom_point() +
  stat_smooth(method = lm)

preds$group=row.names(preds)
colnames(preds)=month.abb
ggplot(preds, aes(group, y.hat2)) +
  geom_point() +
  stat_smooth(method = lm)

#Em surten punts random i no puc fer lo dels intervals de confianÃ§a
```

For both prediction models, the resulting mean squared errors are extremely large Thus, we can state that the results of both predictions are not accurate. This fact is produced due to the small size of the dataset and the number of variables we are working with. IMaybe we should have obtained more reliable results if we have had more variables in our dataset or just a greater number of observations. 

*****
# Conclusions
*****

It is also important to highlight the time in which we data was collected (1854-1856) to better understand the lack of information compiled by Nightingale and the tools he used to process this data.

*****
# Web References
*****
Useful links to learn more about Florence Nightingale and her legacy:

https://theconversation.com/the-healing-power-of-data-florence-nightingales-true-legacy-134649

https://www.history.com/topics/womens-history/florence-nightingale-1

https://mujeresconciencia.com/2017/08/22/florence-nightingale-mucho-mas-la-dama-la-lampara/